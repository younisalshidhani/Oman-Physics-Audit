import streamlit as st
import fitz  # PyMuPDF
import google.generativeai as genai
import json
import re
from docx import Document
from docx.shared import Pt, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from io import BytesIO

# ==========================================
# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„ØªØµÙ…ÙŠÙ…
# ==========================================
st.set_page_config(page_title="Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„ØªØ±Ø¨ÙˆÙŠ Ø§Ù„Ø¹Ù…Ø§Ù†ÙŠ (Pro)", layout="wide")

st.markdown("""
    <style>
    .stApp { direction: rtl; text-align: right; }
    div[data-testid="stSidebar"] { text-align: right; direction: rtl; }
    .header-box { background: #f0f8ff; padding: 20px; border-radius: 10px; border-right: 8px solid #007bff; margin-bottom: 20px; }
    .success-box { background: #d4edda; padding: 15px; border-radius: 5px; color: #155724; border: 1px solid #c3e6cb; }
    table { width: 100%; direction: rtl; border-collapse: collapse; }
    th, td { border: 1px solid #ddd; padding: 8px; text-align: right; }
    th { background-color: #f2f2f2; font-weight: bold; }
    </style>
    """, unsafe_allow_html=True)

# ==========================================
# 2. Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© (Word + PDF)
# ==========================================

def extract_text_from_pdf(uploaded_file):
    if not uploaded_file: return ""
    try:
        doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
        text = "".join([page.get_text() for page in doc])
        return text
    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}"

def create_word_docx(report_data, subject, grade):
    doc = Document()
    
    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    title = doc.add_heading(f'ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø§Ø¯Ø© {subject} - Ø§Ù„ØµÙ {grade}', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph(f'ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆÙÙ‚ Ù…Ø¹Ø§ÙŠÙŠØ± ÙˆØ«ÙŠÙ‚Ø© ØªÙ‚ÙˆÙŠÙ… ØªØ¹Ù„Ù… Ø§Ù„Ø·Ù„Ø¨Ø© - Ø³Ù„Ø·Ù†Ø© Ø¹Ù…Ø§Ù†')
    doc.add_paragraph(f'--------------------------------------------------------')

    # Ø¯Ø§Ù„Ø© Ù„Ø¥Ø¶Ø§ÙØ© Ø¬Ø¯ÙˆÙ„ ÙÙŠ Ø§Ù„ÙˆÙˆØ±Ø¯
    def add_table_to_doc(headers, rows):
        table = doc.add_table(rows=1, cols=len(headers))
        table.style = 'Table Grid'
        table.alignment = WD_ALIGN_PARAGRAPH.RIGHT
        hdr_cells = table.rows[0].cells
        for i, h in enumerate(headers):
            hdr_cells[i].text = h
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¶Ø¨Ø· Ø§Ù„Ø§ØªØ¬Ø§Ù‡ (ØºØ§Ù„Ø¨Ø§Ù‹ Ø§Ù„ÙˆÙˆØ±Ø¯ ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„ØºØ©ØŒ Ù„ÙƒÙ† Ù‡Ø°Ø§ ÙŠÙÙŠ Ø¨Ø§Ù„ØºØ±Ø¶)
        
        for row_data in rows:
            row_cells = table.add_row().cells
            for i, item in enumerate(row_data):
                row_cells[i].text = str(item)
        doc.add_paragraph('') # Ù…Ø³Ø§ÙØ©

    # 1. Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
    doc.add_heading('1. Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†ÙŠØ©', level=1)
    vocab_headers = ["Ø±Ù‚Ù… Ø§Ù„Ø³Ø¤Ø§Ù„", "Ø§Ù„Ù‡Ø¯Ù Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ", "Ø§Ù„Ù…Ø³ØªÙˆÙ‰ (AO1/AO2)", "Ø§Ù„Ø¯Ø±Ø¬Ø©", "Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ÙÙ†ÙŠØ©", "Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ù‚ØªØ±Ø­"]
    vocab_rows = []
    if "vocab_table" in report_data:
        for item in report_data["vocab_table"]:
            vocab_rows.append([
                item.get("q_num", "-"),
                item.get("objective", "-"),
                item.get("level", "-"),
                item.get("marks", "-"),
                item.get("note", "-"),
                item.get("fix", "-")
            ])
        add_table_to_doc(vocab_headers, vocab_rows)

    # 2. Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„
    doc.add_heading('2. Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„ (Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„ÙÙ†ÙŠØ©)', level=1)
    working_headers = ["Ø§Ù„Ø¨Ù†Ø¯", "Ø§Ù„Ù‚ÙŠÙ…Ø© / Ø§Ù„Ø¹Ø¯Ø¯", "Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Ù…Ø·Ø§Ø¨Ù‚/ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚)"]
    working_rows = []
    if "working_table" in report_data:
        wt = report_data["working_table"]
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø©
        items_map = {
            "total_questions": "Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª",
            "lessons_count": "Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ø±ÙˆØ³ Ø§Ù„Ù…ØºØ·Ø§Ø©",
            "ao1_marks": "Ù…Ø¬Ù…ÙˆØ¹ Ø¯Ø±Ø¬Ø§Øª AO1",
            "ao2_marks": "Ù…Ø¬Ù…ÙˆØ¹ Ø¯Ø±Ø¬Ø§Øª AO2",
            "mcq_distractors": "Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø´ØªØªØ§Øª (MCQ)",
            "clarity": "ÙˆØ¶ÙˆØ­ Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª ÙˆØ§Ù„ØµÙŠØ§ØºØ©"
        }
        for key, label in items_map.items():
            val = wt.get(key, {})
            working_rows.append([label, val.get("value", "-"), val.get("status", "-")])
        add_table_to_doc(working_headers, working_rows)

    # 3. Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ø§Ù…
    doc.add_heading('3. Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ø§Ù… ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª', level=1)
    if "summary" in report_data:
        p = doc.add_paragraph(report_data["summary"])
        p.alignment = WD_ALIGN_PARAGRAPH.RIGHT

    # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

# ==========================================
# 3. Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ==========================================

with st.sidebar:
    st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
    api_key = st.text_input("Ù…ÙØªØ§Ø­ API:", type="password")
    subject = st.selectbox("Ø§Ù„Ù…Ø§Ø¯Ø©:", ["ÙÙŠØ²ÙŠØ§Ø¡", "ÙƒÙŠÙ…ÙŠØ§Ø¡", "Ø£Ø­ÙŠØ§Ø¡", "Ø¹Ù„ÙˆÙ…", "Ø±ÙŠØ§Ø¶ÙŠØ§Øª"])
    grade = st.selectbox("Ø§Ù„ØµÙ:", ["10", "11", "12"])
    pages = st.text_input("Ù†Ø·Ø§Ù‚ Ø§Ù„ØµÙØ­Ø§Øª (Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©):", "Ù…Ø«Ù„Ø§Ù‹: 12-45")

st.markdown('<div class="header-box"><h2>ğŸ‡´ğŸ‡² Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª - Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¹Ù…Ø§Ù†ÙŠØ©</h2><p>ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØµØ¯ÙŠØ± Ù„Ù…Ù„Ù Word + ØªØ­Ù„ÙŠÙ„ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©</p></div>', unsafe_allow_html=True)

col1, col2, col3 = st.columns(3)
with col1: t_file = st.file_uploader("1. Ù…Ù„Ù Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (PDF)", type="pdf")
with col2: p_file = st.file_uploader("2. ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ… (PDF)", type="pdf")
with col3: b_file = st.file_uploader("3. ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ (PDF)", type="pdf")

if st.button("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±") and api_key and t_file:
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ 30 Ø«Ø§Ù†ÙŠØ©)..."):
            # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
            txt_test = extract_text_from_pdf(t_file)
            txt_book = extract_text_from_pdf(b_file)
            txt_policy = extract_text_from_pdf(p_file)
            
            # 2. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª (Ù…Ø­ÙƒÙ… Ø¬Ø¯Ø§Ù‹ Ù„ÙŠØ®Ø±Ø¬ JSON)
            prompt = f"""
            Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªÙ‚ÙˆÙŠÙ… ØªØ±Ø¨ÙˆÙŠ ÙÙŠ ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ±Ø¨ÙŠØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø³Ù„Ø·Ù†Ø© Ø¹Ù…Ø§Ù†.
            Ù„Ø¯ÙŠÙƒ Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù…Ø§Ø¯Ø© {subject} Ù„Ù„ØµÙ {grade}.
            
            Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆÙ…Ù‚Ø§Ø±Ù†ØªÙ‡ Ø¨Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ (ØµÙØ­Ø§Øª {pages}) ÙˆØ¨ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ….
            
            Ø§Ù„Ù…Ù‡Ù…Ø©: Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†ØµÙˆØµ Ø¥Ø¶Ø§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø£Ùˆ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©).
            
            Ù‡ÙŠÙƒÙ„ JSON Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
            {{
                "vocab_table": [
                    {{ "q_num": "1", "objective": "ÙˆØµÙ Ø§Ù„Ù‡Ø¯Ù", "level": "AO1", "marks": "2", "note": "Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©", "fix": "Ø§Ù„ØªØ¹Ø¯ÙŠÙ„" }},
                    ... Ù„ÙƒÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
                ],
                "working_table": {{
                    "total_questions": {{ "value": "Ø±Ù‚Ù…", "status": "Ù…Ø·Ø§Ø¨Ù‚/ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚" }},
                    "lessons_count": {{ "value": "ØªÙ‚Ø¯ÙŠØ±ÙŠ", "status": "-" }},
                    "ao1_marks": {{ "value": "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹", "status": "-" }},
                    "ao2_marks": {{ "value": "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹", "status": "-" }},
                    "mcq_distractors": {{ "value": "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´ØªØªØ§Øª", "status": "Ø¬ÙŠØ¯/Ø¶Ø¹ÙŠÙ" }},
                    "clarity": {{ "value": "ÙˆØµÙ Ø§Ù„ÙˆØ¶ÙˆØ­", "status": "-" }}
                }},
                "summary": "Ø§ÙƒØªØ¨ Ù‡Ù†Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø®ØªØ§Ù…ÙŠ ÙˆÙ†Ø³Ø¨Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©."
            }}

            Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©:
            - Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {txt_test[:10000]}
            - Ø§Ù„ÙƒØªØ§Ø¨: {txt_book[:10000]} (Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯Ø±ÙˆØ³ ÙˆØ§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª)
            - Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©: {txt_policy[:5000]} (Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ±)
            """

            # 3. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
            response = model.generate_content(prompt)
            clean_json = response.text.replace("```json", "").replace("```", "").strip()
            
            try:
                data = json.loads(clean_json)
                
                # --- Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ---
                
                # 1. Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
                st.subheader("1. Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª")
                html_table = "<table><tr><th>Ø§Ù„Ø³Ø¤Ø§Ù„</th><th>Ø§Ù„Ù‡Ø¯Ù</th><th>Ø§Ù„Ù…Ø³ØªÙˆÙ‰</th><th>Ø§Ù„Ø¯Ø±Ø¬Ø©</th><th>Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©</th><th>Ø§Ù„ØªØ¹Ø¯ÙŠÙ„</th></tr>"
                for row in data.get("vocab_table", []):
                    html_table += f"<tr><td>{row.get('q_num')}</td><td>{row.get('objective')}</td><td>{row.get('level')}</td><td>{row.get('marks')}</td><td>{row.get('note')}</td><td>{row.get('fix')}</td></tr>"
                html_table += "</table>"
                st.markdown(html_table, unsafe_allow_html=True)

                # 2. Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„
                st.subheader("2. Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
                wt = data.get("working_table", {})
                html_w_table = "<table><tr><th>Ø§Ù„Ø¨Ù†Ø¯</th><th>Ø§Ù„Ù‚ÙŠÙ…Ø© / Ø§Ù„ÙˆØµÙ</th><th>Ø§Ù„Ø­Ø§Ù„Ø©</th></tr>"
                labels = {
                    "total_questions": "Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©", "lessons_count": "Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ø±ÙˆØ³", 
                    "ao1_marks": "Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù…Ø¹Ø±ÙØ© (AO1)", "ao2_marks": "Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚/Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (AO2)",
                    "mcq_distractors": "Ù…Ø´ØªØªØ§Øª Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯", "clarity": "Ø¬ÙˆØ¯Ø© Ø§Ù„Ø±Ø³ÙˆÙ… ÙˆØ§Ù„ØµÙŠØ§ØºØ©"
                }
                for k, label in labels.items():
                    item = wt.get(k, {})
                    html_w_table += f"<tr><td>{label}</td><td>{item.get('value')}</td><td>{item.get('status')}</td></tr>"
                html_w_table += "</table>"
                st.markdown(html_w_table, unsafe_allow_html=True)

                # 3. Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ø§Ù…
                st.subheader("3. Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ø§Ù…")
                st.info(data.get("summary", "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ"))

                # 4. Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Word)
                docx_file = create_word_docx(data, subject, grade)
                st.download_button(
                    label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨ØµÙŠØºØ© Word (.docx)",
                    data=docx_file,
                    file_name="Oman_Exam_Report.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )

            except json.JSONDecodeError:
                # ÙÙŠ Ø­Ø§Ù„ ÙØ´Ù„ Ø§Ù„Ù€ JSONØŒ Ù†Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ ÙƒÙ…Ø§ Ø¬Ø§Ø¡ (Ø®Ø·Ø© Ø¨Ø¯ÙŠÙ„Ø©)
                st.warning("ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ØŒ Ù„ÙƒÙ† Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ. Ø¥Ù„ÙŠÙƒ Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù…:")
                st.markdown(response.text)

    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}")
        st.info("Ù†ØµÙŠØ­Ø©: ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ù‡ÙŠ Ù…Ù„ÙØ§Øª PDF ØµØ§Ù„Ø­Ø© ÙˆÙ„ÙŠØ³Øª ØµÙˆØ±Ø§Ù‹.")
