import re
import json
from io import BytesIO
from typing import List, Dict, Tuple

import streamlit as st
import fitz  # PyMuPDF
import google.generativeai as genai

from docx import Document
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.table import WD_TABLE_ALIGNMENT

try:
    from PIL import Image
except Exception:
    Image = None


# =========================
# ÙˆØ§Ø¬Ù‡Ø©
# =========================
st.set_page_config(page_title="Ù†Ø¸Ø§Ù… ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª - Ø³Ù„Ø·Ù†Ø© Ø¹Ù…Ø§Ù†", layout="wide")

st.markdown(
    """
    <style>
    .stApp { direction: rtl; text-align: right; }
    div[data-testid="stSidebar"] { text-align: right; direction: rtl; }
    div[data-testid="stMarkdownContainer"] { text-align: right; direction: rtl; }

    .report-box { border: 2px solid #007bff; padding: 14px; border-radius: 10px; background-color: #f9f9f9; }
    .tbl { width:100%; border-collapse:collapse; direction:rtl; }
    .tbl th, .tbl td { border:1px solid #ddd; padding:8px; vertical-align:top; text-align:right; }
    .tbl th { background:#f1f1f1; font-weight:700; }
    .muted { color:#666; font-size: 13px; }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ” Ù†Ø¸Ø§Ù… ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª (Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¨Ù†ÙˆØ¯ ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±)")
st.caption("ÙŠØ±ÙØ¹: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± + ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ… + ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ØŒ Ø«Ù… ÙŠØ¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙØ­Ø© ÙˆÙÙ‚ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ Ù…Ø¹ Ø®ÙŠØ§Ø± ØªÙ†Ø²ÙŠÙ„ Word.")


# =========================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¬Ø§Ù†Ø¨ÙŠØ©
# =========================
st.sidebar.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚")
api_key = st.sidebar.text_input("Ù…ÙØªØ§Ø­ API (Gemini):", type="password")

subject = st.sidebar.selectbox("Ø§Ù„Ù…Ø§Ø¯Ø©:", ["ÙÙŠØ²ÙŠØ§Ø¡", "ÙƒÙŠÙ…ÙŠØ§Ø¡", "Ø£Ø­ÙŠØ§Ø¡", "Ø¹Ù„ÙˆÙ…"], index=0)
semester = st.sidebar.selectbox("Ø§Ù„ÙØµÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ:", ["Ø§Ù„Ø£ÙˆÙ„", "Ø§Ù„Ø«Ø§Ù†ÙŠ"], index=1)
grade = st.sidebar.selectbox("Ø§Ù„ØµÙ:", ["11", "12"], index=1)
exam_type = st.sidebar.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:", ["Ù‚ØµÙŠØ±", "Ø§Ø³ØªÙ‚ØµØ§Ø¦ÙŠ"], index=0)

# Ù†Ø·Ø§Ù‚ Ø§Ù„ØµÙØ­Ø§Øª Ø®Ø§Øµ Ø¨ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ ÙÙ‚Ø·
pages_range = st.sidebar.text_input("Ù†Ø·Ø§Ù‚ ØµÙØ­Ø§Øª ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ (Ù…Ø«Ø§Ù„ 77-97):", value="")


# =========================
# ØªØ¹Ø±ÙŠÙØ§Øª Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø±Ø³Ù…ÙŠØ©
# =========================
A01_DEFINITION = """
Ù‡Ø¯Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙˆÙ„ (A01): Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„ÙÙ‡Ù….
ÙŠÙ‚ÙŠØ³ ØªØ°ÙƒØ± ÙˆÙÙ‡Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆØ§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ù‡Ø§ØŒ ÙˆØªÙØ³ÙŠØ±Ù‡Ø§ Ø£Ùˆ ØªÙˆØ¶ÙŠØ­Ù‡Ø§ Ø¨ØµÙˆØ±Ø© Ù…Ø¨Ø³Ø·Ø©.
ÙŠÙ…Ø«Ù„ Ø°Ù„Ùƒ: Ø§Ù„Ù…Ø¹Ø·ÙŠØ§Øª ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚ ÙˆØ§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„ØªØ¹Ø±ÙŠÙØ§Øª ÙˆØ§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ©ØŒ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„ÙˆØ­Ø¯Ø§Øª ÙˆØ§Ù„ØµÙŠØº ÙˆØ§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ©ØŒ
Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ØªØ®Ø·ÙŠØ·ÙŠØ©/Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ø§Ù„ÙˆØ§Ø¶Ø­Ø©ØŒ Ø§Ù„Ø¸ÙˆØ§Ù‡Ø± ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§ØªØŒ Ø®ÙˆØ§Øµ Ø§Ù„Ù…ÙˆØ§Ø¯ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©ØŒ Ø§Ù„ÙƒÙ…ÙŠØ§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ© ÙˆÙ‚ÙŠØ§Ø³Ù‡Ø§.

ÙˆÙŠØªØ·Ù„Ø¨ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ© Ù…Ø«Ù„:
- Ø¥Ø¬Ø±Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ§Øª Ø­Ø³Ø§Ø¨ÙŠØ© Ø°Ø§Øª Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©.
- Ø¥Ø¬Ø±Ø§Ø¡ ØªØ¹ÙˆÙŠØ¶ Ø¨Ø³ÙŠØ· Ù„Ù„Ø£Ø±Ù‚Ø§Ù… ÙÙŠ ØµÙŠØºØ© ÙŠØªÙ… ØªØ°ÙƒØ±Ù‡Ø§ Ø£Ùˆ ØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§.
- Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø¨Ø³ÙŠØ·Ø©/Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø³ÙŠØ·Ø© Ù„Ù„ØµÙŠØº Ø£Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.
"""

A02_DEFINITION = """
Ù‡Ø¯Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø«Ø§Ù†ÙŠ (A02): Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ….
ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø£Ù„ÙˆÙØ© Ù„Ø¯Ù‰ Ø§Ù„Ø·Ù„Ø¨Ø©ØŒ Ø¨Ù…Ø§ ÙŠØªØ·Ù„Ø¨ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆØ§Ø³ØªÙ†ØªØ§Ø¬ÙŠØ©ØŒ
ÙˆÙŠÙØªÙˆÙ‚Ø¹ Ø£Ù† ÙŠØ·Ù„Ø¨ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø£Ùˆ ØªÙ‚ÙŠÙŠÙ…Ù‡Ø§ØŒ ÙˆÙ‚Ø¯ ÙŠØµÙ„ Ù„Ù…Ø³ØªÙˆÙ‰ Ø£Ø¹Ù…Ù‚ Ù…Ù† Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù†Ù‚Ø¯ÙŠ.

ÙŠÙ…Ø«Ù„ Ø°Ù„Ùƒ: Ø¹Ø±Ø¶/ØªÙØ³ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø´ÙƒÙ„ Ù…Ø±Ø¦ÙŠ (Ø¬Ø¯Ø§ÙˆÙ„/Ø±Ø³ÙˆÙ…Ø§Øª/ØµÙˆØ±/Ù…Ø®Ø·Ø·Ø§Øª/ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø¨ÙŠØ§Ù†ÙŠØ©)ØŒ
Ø¬Ù…Ø¹ ÙˆØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ Ø¨ØµÙˆØ±Ø© Ø¹Ù„Ù…ÙŠØ©ØŒ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ
Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù‚ÙŠÙ‚Ø§Øª/ØªØ¬Ø§Ø±Ø¨ ÙˆØ¯Ø¹Ù… Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª ÙˆØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŒ Ø±Ø¨Ø· Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ø³ÙŠØ§Ù‚Ø§Øª ØºÙŠØ± Ù…Ø£Ù„ÙˆÙØ©ØŒ
Ø´Ø±Ø­ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ÙˆØ§Ù„Ø¸ÙˆØ§Ù‡Ø± ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª ØªÙØ³ÙŠØ±Ù‹Ø§ Ø³Ø¨Ø¨ÙŠÙ‹Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª/Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø¥Ø«Ø¨Ø§Øª Ø§Ù„Ù…ÙÙ‡ÙˆÙ…ØŒ
Ø­Ø³Ø§Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© (Ø®ØµÙˆØµÙ‹Ø§ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø®Ø·ÙˆØ§Øª)ØŒ Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª.
"""


# =========================
# Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
# =========================
col1, col2, col3 = st.columns(3)
with col1:
    file_test = st.file_uploader("1) Ù…Ù„Ù Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (PDF)", type="pdf")
with col2:
    file_policy = st.file_uploader("2) ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ… (PDF)", type="pdf")
with col3:
    file_book = st.file_uploader("3) ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ (PDF)", type="pdf")


# =========================
# Ø£Ø¯ÙˆØ§Øª PDF + Ù†Øµ + JSON
# =========================
def _normalize_dash(s: str) -> str:
    return re.sub(r"[â€“â€”âˆ’]", "-", (s or "").strip())

def _parse_page_range(rng: str):
    if not rng or not rng.strip():
        return None
    s = _normalize_dash(rng)
    m = re.match(r"^\s*(\d+)\s*-\s*(\d+)\s*$", s)
    if not m:
        return None
    a, b = int(m.group(1)), int(m.group(2))
    if a <= 0 or b <= 0:
        return None
    if a > b:
        a, b = b, a
    return (a, b)

@st.cache_data(show_spinner=False)
def extract_text_from_pdf_textonly(pdf_bytes: bytes, page_range_1idx=None) -> str:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    start0, end0 = 0, doc.page_count - 1
    if page_range_1idx:
        a, b = page_range_1idx
        start0 = max(0, a - 1)
        end0 = min(doc.page_count - 1, b - 1)

    parts = []
    for i in range(start0, end0 + 1):
        page = doc.load_page(i)
        parts.append(page.get_text("text"))
    doc.close()
    return "\n".join(parts).strip()

def safe_clip(text: str, max_chars: int) -> str:
    return (text or "")[:max_chars]

def strip_control_chars(s: str) -> str:
    if not s:
        return ""
    return re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", "", s)

def repair_json_text(s: str) -> str:
    s = strip_control_chars(s).strip()
    s = re.sub(r"^```(?:json)?\s*", "", s)
    s = re.sub(r"\s*```$", "", s)

    start = s.find("{")
    end = s.rfind("}")
    if start != -1 and end != -1 and end > start:
        s = s[start:end + 1]

    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€", '"').replace("â€™", "'").replace("â€˜", "'")
    s = s.replace("ØŒ", ",")
    s = re.sub(r",\s*([}\]])", r"\1", s)

    s = re.sub(r'(")\s*\n\s*(")', r'\1,\n\2', s)
    s = re.sub(r'(\d)\s*\n\s*(")', r'\1,\n\2', s)
    s = re.sub(r'(})\s*\n\s*(")', r'\1,\n\2', s)
    s = re.sub(r'(])\s*\n\s*(")', r'\1,\n\2', s)
    return s

def parse_json_robust(raw: str) -> dict:
    if not raw:
        raise ValueError("Ø±Ø¯ ÙØ§Ø±Øº Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    raw = strip_control_chars(raw)
    try:
        return json.loads(raw)
    except Exception:
        fixed = repair_json_text(raw)
        return json.loads(fixed)


# =========================
# Ø§Ø®ØªÙŠØ§Ø± Ù…ÙˆØ¯ÙŠÙ„ (Ù„ØªÙØ§Ø¯ÙŠ 404)
# =========================
def pick_model(preferred="gemini-2.5-flash"):
    models = [
        m for m in genai.list_models()
        if "generateContent" in getattr(m, "supported_generation_methods", [])
    ]
    names = [m.name for m in models]  # models/...

    pref = preferred if preferred.startswith("models/") else f"models/{preferred}"
    if pref in names:
        return genai.GenerativeModel(pref), pref

    for n in names:
        if "flash" in n and "preview" not in n:
            return genai.GenerativeModel(n), n

    return genai.GenerativeModel(names[0]), names[0]


# =========================
# OCR Ø¹Ø¨Ø± Gemini (Ø¹Ù†Ø¯ ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø£Ùˆ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ØºÙŠØ± Ù…ÙÙŠØ¯)
# =========================
def _page_indices(doc, page_range_1idx):
    start0, end0 = 0, doc.page_count - 1
    if page_range_1idx:
        a, b = page_range_1idx
        start0 = max(0, a - 1)
        end0 = min(doc.page_count - 1, b - 1)
    return list(range(start0, end0 + 1))

def _is_text_meaningful(txt: str) -> bool:
    if not txt:
        return False
    t = strip_control_chars(txt)
    if len(t.strip()) < 300:
        return False

    ar = len(re.findall(r"[\u0600-\u06FF]", t))
    dg = len(re.findall(r"\d", t))

    if ar < 120 and dg < 30:
        return False

    has_qmark = "ØŸ" in t or "?" in t
    has_numbering = bool(re.search(r"(^|\n)\s*\d+\s*[-.)]", t))
    has_keyword = bool(re.search(r"(Ø³Ø¤Ø§Ù„|Ø§Ø®ØªØ±|Ø£Ø¬Ø¨|Ø¹Ù„Ù„|ÙØ³Ø±|Ø§Ø­Ø³Ø¨|Ø§ÙˆØ¬Ø¯)", t))
    return has_qmark or has_numbering or has_keyword or (ar > 300)

def ocr_pdf_with_gemini(model, pdf_bytes: bytes, page_range_1idx=None, max_pages: int = 25) -> str:
    if Image is None:
        raise RuntimeError("Pillow ØºÙŠØ± Ù…ØªÙˆÙØ±. Ø£Ø¶Ù pillow Ø¥Ù„Ù‰ requirements.txt")
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages = _page_indices(doc, page_range_1idx)

    if len(pages) > max_pages:
        pages = pages[:max_pages]

    out_parts = []
    ocr_prompt = (
        "Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ø¸Ø§Ù‡Ø± ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ/Ø£Ø±Ù‚Ø§Ù…/Ø±Ù…ÙˆØ²). "
        "Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ ÙÙ‚Ø· ÙƒÙ…Ø§ Ù‡Ùˆ Ø¯ÙˆÙ† Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ©. "
        "Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø³Ø·ÙˆØ±. "
        "Ù„Ø§ ØªØ¶Ù Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©."
    )

    for pno in pages:
        page = doc.load_page(pno)
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2), alpha=False)
        png_bytes = pix.tobytes("png")
        img = Image.open(BytesIO(png_bytes))

        resp = model.generate_content(
            [ocr_prompt, img],
            generation_config={"temperature": 0.0, "top_p": 1.0, "max_output_tokens": 2048},
        )
        txt = (getattr(resp, "text", "") or "").strip()
        if txt:
            out_parts.append(txt)

    doc.close()
    return "\n\n".join(out_parts).strip()

def extract_text_auto(model, pdf_bytes: bytes, page_range_1idx=None) -> Tuple[str, str]:
    txt = extract_text_from_pdf_textonly(pdf_bytes, page_range_1idx)
    if _is_text_meaningful(txt):
        return txt, "text"

    txt_ocr = ocr_pdf_with_gemini(model, pdf_bytes, page_range_1idx, max_pages=25)
    if txt_ocr.strip():
        return txt_ocr, "ocr"

    return txt, "text"


# =========================
# Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø³ÙŠØ§Ù‚ Ù…Ù† ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ…/Ø§Ù„ÙƒØªØ§Ø¨
# =========================
_ARABIC_DIACRITICS = re.compile(r"[\u064B-\u065F\u0670\u06D6-\u06ED]")

def norm_ar(s: str) -> str:
    s = s or ""
    s = _ARABIC_DIACRITICS.sub("", s)
    s = s.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    s = s.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def chunk_text(text: str, chunk_size: int = 1400, overlap: int = 250) -> List[str]:
    text = text or ""
    if len(text) <= chunk_size:
        return [text]
    chunks = []
    i = 0
    while i < len(text):
        j = min(len(text), i + chunk_size)
        chunks.append(text[i:j])
        i = max(j - overlap, i + 1)
    return chunks

def score_overlap(query: str, chunk: str) -> float:
    q = set(norm_ar(query).split())
    c = set(norm_ar(chunk).split())
    if not q or not c:
        return 0.0
    inter = len(q & c)
    return inter / (len(q) + 1e-9)

def top_k_chunks(query: str, text: str, k: int = 4) -> List[str]:
    chunks = chunk_text(text, chunk_size=1400, overlap=250)
    scored = sorted(((score_overlap(query, ch), ch) for ch in chunks), key=lambda x: x[0], reverse=True)
    best = [ch for sc, ch in scored[:k] if sc > 0]
    return best if best else (chunks[:k] if chunks else [])


# =========================
# Ù…Ø±Ø´Ù‘Ø­ Ø£ÙˆÙ„ÙŠ A01/A02
# =========================
A02_TRIGGERS = [
    "Ø§Ø³ØªÙ†ØªØ¬", "Ø­Ù„Ù„", "Ù‚Ø§Ø±Ù†", "Ø¹Ù„Ù„", "ÙØ³Ø±", "Ø¨Ø±Ø±", "Ù†Ø§Ù‚Ø´", "Ø§Ø«Ø¨Øª", "Ø¨Ø±Ù‡Ù†",
    "Ù…Ù† Ø§Ù„Ø±Ø³Ù…", "Ù…Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„", "Ø§Ø±Ø³Ù…", "Ù…Ø«Ù„ Ø¨ÙŠØ§Ù†ÙŠØ§", "Ù…Ù†Ø­Ù†Ù‰", "Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ", "Ù…Ø®Ø·Ø·", "Ø¨ÙŠØ§Ù†Ø§Øª",
    "ØªØ¬Ø±Ø¨Ø©", "ØªØ­Ù‚ÙŠÙ‚", "Ø§Ø³ØªÙ‚ØµØ§Ø¡", "ØµÙ…Ù…", "Ø§Ù‚ØªØ±Ø­", "ØªÙˆÙ‚Ø¹", "Ø§Ø³ØªØ®Ù„Øµ", "ÙØ³Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬",
    "Ù…ØªØ¹Ø¯Ø¯Ø©", "Ø®Ø·ÙˆØ§Øª", "ØªÙ‚ÙŠÙŠÙ…", "Ø³ÙŠØ§Ù‚ ØºÙŠØ± Ù…Ø£Ù„ÙˆÙ"
]
A01_TRIGGERS = [
    "Ø¹Ø±Ù", "Ø§Ø°ÙƒØ±", "Ø¹Ø¯Ø¯", "Ø³Ù…", "Ù…Ø§ Ø§Ù„Ù…Ù‚ØµÙˆØ¯", "Ù…Ø§ Ù‡Ùˆ", "Ø­Ø¯Ø¯", "ØµÙ", "ÙˆØ¶Ø­", "Ø§ÙƒØªØ¨", "Ø¨ÙŠÙ† Ù…Ø¹Ù†Ù‰",
    "Ù‚Ø§Ù†ÙˆÙ†", "ÙˆØ­Ø¯Ø©", "Ø±Ù…Ø²", "Ù…ØµØ·Ù„Ø­", "ØªØ¹Ø±ÙŠÙ"
]

def heuristic_assessment_objective(item_text: str) -> str:
    t = norm_ar(item_text)

    for w in A02_TRIGGERS:
        if norm_ar(w) in t:
            return "A02"

    for w in A01_TRIGGERS:
        if norm_ar(w) in t:
            return "A01"

    if re.search(r"(Ù…Ù† Ø§Ù„Ø±Ø³Ù…|Ù…Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„|Ø¨ÙŠØ§Ù†Ø§Øª|Ù…Ù†Ø­Ù†Ù‰|Ù…Ø®Ø·Ø·)", item_text):
        return "A02"

    return "A01"


# =========================
# LLM: ØªÙˆÙ„ÙŠØ¯ JSON + ØªØµØ­ÙŠØ­
# =========================
def generate_json(model, prompt: str, tries: int = 3) -> Tuple[dict, str]:
    last_raw = ""
    last_err = ""

    base_cfg = {
        "temperature": 0.0,
        "top_p": 1.0,
        "max_output_tokens": 8192,
    }

    for attempt in range(1, tries + 1):
        cfg = dict(base_cfg)
        if attempt == 1:
            cfg["response_mime_type"] = "application/json"

        try:
            resp = model.generate_content(prompt, generation_config=cfg)
        except TypeError:
            cfg.pop("response_mime_type", None)
            resp = model.generate_content(prompt, generation_config=cfg)

        last_raw = getattr(resp, "text", "") or ""

        try:
            return parse_json_robust(last_raw), last_raw
        except Exception as e:
            last_err = str(e)
            snippet = safe_clip(last_raw, 25000)
            prompt = f"""
Ø£ØµÙ„Ø­ JSON Ø§Ù„ØªØ§Ù„ÙŠ Ù„ÙŠØµØ¨Ø­ JSON ØµØ§Ù„Ø­ 100% ÙˆÙ„Ø§ ØªÙƒØªØ¨ Ø£ÙŠ Ø´ÙŠØ¡ ØºÙŠØ± JSON.
Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:
- Ø§Ø³ØªØ®Ø¯Ù… " Ù„Ù„Ù…ÙØ§ØªÙŠØ­ ÙÙ‚Ø·
- Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… " Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‚ÙŠÙ… (Ø§Ø³ØªØ¨Ø¯Ù„Ù‡Ø§ Ø¨Ù€ ')
- Ù„Ø§ ØªØ¶Ø¹ Ø£Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‚ÙŠÙ…
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙØ§ØµÙ„Ø© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© , ÙÙ‚Ø·

JSON ØºÙŠØ± ØµØ§Ù„Ø­:
{snippet}
"""

    raise ValueError(f"ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ JSON ØµØ§Ù„Ø­. Ø¢Ø®Ø± Ø®Ø·Ø£: {last_err}")


# =========================
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
# =========================
def extract_items_via_llm(model, txt_test: str) -> List[Dict]:
    prompt = f"""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ØªØ¯Ù‚ÙŠÙ‚ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª.
Ù…Ù‡Ù…ØªÙƒ: Ø§Ø³ØªØ®Ø±Ø§Ø¬ "Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª/Ø§Ù„Ø£Ø³Ø¦Ù„Ø©" Ù…Ù† Ù†Øµ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¥Ø±Ø¬Ø§Ø¹ JSON ÙÙ‚Ø·.

Ù‚ÙˆØ§Ø¹Ø¯:
- JSON ÙÙ‚Ø·.
- Ù„ÙƒÙ„ Ù…ÙØ±Ø¯Ø©: Ø±Ù‚Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø© (number) + Ù†Øµ Ø§Ù„Ù…ÙØ±Ø¯Ø© (text) + Ø§Ù„Ø¯Ø±Ø¬Ø© Ø¥Ù† ÙˆÙØ¬Ø¯Øª (marks ÙƒÙ†Øµ).
- Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¸Ù‡Ø± Ø±Ù‚Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø© Ø¨ÙˆØ¶ÙˆØ­: Ø£Ù†Ø´Ø¦ ØªØ±Ù‚ÙŠÙ…Ù‹Ø§ Ù…ØªØ³Ù„Ø³Ù„Ù‹Ø§ 1..n.
- Ù„Ø§ ØªØ¶Ø¹ " Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‚ÙŠÙ….

ØµÙŠØºØ© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬:
{{
  "items":[
    {{"number":"1","text":"...","marks":"1"}}
  ]
}}

Ù†Øµ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:
{safe_clip(txt_test, 90000)}
"""
    data, _ = generate_json(model, prompt, tries=3)
    items = data.get("items", []) or []

    cleaned = []
    seq = 1
    for it in items:
        num = str(it.get("number", "")).strip() or str(seq)
        txt = str(it.get("text", "")).strip()
        marks = str(it.get("marks", "")).strip()
        if txt:
            cleaned.append({"number": num, "text": txt, "marks": marks})
            seq += 1
    return cleaned


# =========================
# ØªØ­Ù„ÙŠÙ„ Ù…ÙØ±Ø¯Ø© ÙˆØ§Ø­Ø¯Ø©
# =========================
def analyze_one_item(model, item: Dict, policy_text: str, book_text: str) -> Dict:
    item_no = str(item.get("number", "")).strip()
    item_text = str(item.get("text", "")).strip()
    item_marks = str(item.get("marks", "")).strip() or "-"

    policy_snips = top_k_chunks(item_text, policy_text, k=4)
    book_snips = top_k_chunks(item_text, book_text, k=3)

    ao_hint = heuristic_assessment_objective(item_text)

    prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªÙ‚ÙˆÙŠÙ… ÙˆÙÙ‚ ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ….
Ø­Ù„Ù‘Ù„ Ù…ÙØ±Ø¯Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ÙˆØ£Ø®Ø±Ø¬ JSON ÙÙ‚Ø·.

Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø±Ø³Ù…ÙŠ (Ø§Ù„ØªØ²Ù… Ø¨Ù‡ Ø­Ø±ÙÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ø§Ø®ØªÙŠØ§Ø± Ù‡Ø¯Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ…):
{A01_DEFINITION}

{A02_DEFINITION}

Ù‚Ø§Ø¹Ø¯Ø© Ù‚Ø±Ø§Ø± ÙˆØ§Ø¶Ø­Ø©:
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: ØªØ·Ø¨ÙŠÙ‚/ØªØ­Ù„ÙŠÙ„/Ø§Ø³ØªÙ†ØªØ§Ø¬/ØªÙØ³ÙŠØ± Ø³Ø¨Ø¨ÙŠ/Ù…Ù‚Ø§Ø±Ù†Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ©/ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ø±Ø³ÙˆÙ…/Ù†ØªØ§Ø¦Ø¬/ØªØ¬Ø±Ø¨Ø©/Ø§Ø³ØªÙ‚ØµØ§Ø¡/Ø­Ù„ Ù…Ø´ÙƒÙ„Ø©/Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø®Ø·ÙˆØ§Øª â†’ A02
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: ØªØ°ÙƒØ±/ØªØ¹Ø±ÙŠÙ/Ø°ÙƒØ±/ØªØ¹Ø¯Ø§Ø¯/ØªØ­Ø¯ÙŠØ¯/ØªØ³Ù…ÙŠØ©/ÙˆØµÙ/ØªÙˆØ¶ÙŠØ­ Ø¨Ø³ÙŠØ·/Ù‚Ø§Ù†ÙˆÙ† Ø£Ùˆ ÙˆØ­Ø¯Ø© Ø£Ùˆ Ø±Ù…Ø²/ØªØ¹ÙˆÙŠØ¶ Ø¨Ø³ÙŠØ·/Ø¹Ù…Ù„ÙŠØ© Ø®Ø·ÙˆØ© ÙˆØ§Ø­Ø¯Ø© â†’ A01
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† ØªØ°ÙƒØ± + ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ ÙŠØ·Ù„Ø¨ Ø¬Ø²Ø¡Ù‹Ø§ Ø¨Ø³ÙŠØ·Ù‹Ø§ Ø«Ù… ØªØ·Ø¨ÙŠÙ‚Ù‹Ø§ â†’ A01/A02

Ù…Ù„Ø§Ø­Ø¸Ø©: Ù…Ø±Ø´Ø­ Ø£ÙˆÙ„ÙŠ (Ù„ÙŠØ³ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§): {ao_hint}

Ù‚ÙˆØ§Ø¹Ø¯ Ø¥Ø®Ø±Ø§Ø¬ ØµØ§Ø±Ù…Ø©:
- JSON ÙÙ‚Ø· Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ.
- Ù„Ø§ ØªØ¶Ø¹ " Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‚ÙŠÙ… (Ø§Ø³ØªØ¨Ø¯Ù„Ù‡Ø§ Ø¨Ù€ ' Ø¥Ù† Ø§Ø­ØªØ¬Øª).
- Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù‚ÙŠÙ… Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.
- learning_objective: Ø§Ø®ØªØ± Ø¹Ø¨Ø§Ø±Ø©/Ø¨Ù†Ø¯ Ù…Ù† ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ… "ÙƒÙ…Ø§ Ù‡Ùˆ" Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù† Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø±ÙÙ‚Ø©ØŒ ÙˆÙ„Ø§ ØªØ®ØªØ± Ù…Ù† Ø®Ø§Ø±Ø¬Ù‡Ø§ Ø¥Ù„Ø§ Ø¹Ù†Ø¯ Ø§Ù„Ø¶Ø±ÙˆØ±Ø© (ÙˆØ¹Ù†Ø¯Ù‡Ø§ Ø¶Ø¹ '-').

ØµÙŠØºØ© JSON Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
{{
  "mufrada": "{item_no}",
  "learning_objective": "...",
  "assessment_objective": "A01 Ø£Ùˆ A02 Ø£Ùˆ A01/A02",
  "marks": "{item_marks}",
  "note_type": "ØµÙŠØ§ØºØ© Ø£Ùˆ Ø¹Ù„Ù…ÙŠØ© Ø£Ùˆ ÙÙ†ÙŠØ© ØªØ´Ù…Ù„ Ø§Ù„Ø±Ø³Ù… Ø£Ùˆ Ù„Ø§ ØªÙˆØ¬Ø¯",
  "note": "...",
  "edit": "...",
  "ao_reason": "Ø³Ø¨Ø¨ Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…Ø®ØªØµØ± Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± (Ù†ÙˆØ¹ Ù…Ù‡Ø§Ø±Ø©/Ù…Ø·Ù„ÙˆØ¨)"
}}

Ø§Ù„Ù…ÙØ±Ø¯Ø© Ø±Ù‚Ù… {item_no}:
{item_text}

Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ… (Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø©):
{chr(10).join([f"- {s}" for s in policy_snips])}

Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ (Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø©):
{chr(10).join([f"- {s}" for s in book_snips])}
"""
    out, raw = generate_json(model, prompt, tries=3)

    out["mufrada"] = item_no

    allowed = {"A01", "A02", "A01/A02"}
    ao = str(out.get("assessment_objective", "")).strip()
    if ao not in allowed:
        out["assessment_objective"] = ao_hint

    strong_a02 = bool(re.search(r"(Ù…Ù† Ø§Ù„Ø±Ø³Ù…|Ù…Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„|Ø¨ÙŠØ§Ù†Ø§Øª|Ù…Ù†Ø­Ù†Ù‰|Ù…Ø®Ø·Ø·|Ø§Ø³ØªÙ†ØªØ¬|Ø­Ù„Ù„|Ø¹Ù„Ù„|ÙØ³Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬|Ø§Ø±Ø³Ù…|Ù…Ø«Ù„ Ø¨ÙŠØ§Ù†ÙŠØ§)", item_text))
    if strong_a02 and str(out.get("assessment_objective", "")).strip() == "A01":
        fix_prompt = f"""
Ø±Ø§Ø¬Ø¹ ØªØµÙ†ÙŠÙ Ù‡Ø¯Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… ÙÙ‚Ø· ÙˆÙÙ‚ Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø±Ø³Ù…ÙŠ.
Ø£Ø®Ø±Ø¬ JSON ÙÙ‚Ø·: {{"assessment_objective":"A01/A02","ao_reason":"..."}}

Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø±Ø³Ù…ÙŠ:
{A01_DEFINITION}
{A02_DEFINITION}

Ø§Ù„Ù…ÙØ±Ø¯Ø©:
{item_text}

Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø­Ø§Ù„ÙŠ: A01
Ø£Ø¹Ø¯ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ø¯Ù‚Ø© Ø´Ø¯ÙŠØ¯Ø©.
"""
        try:
            fix, _ = generate_json(model, fix_prompt, tries=2)
            new_ao = str(fix.get("assessment_objective", "")).strip()
            if new_ao in allowed:
                out["assessment_objective"] = new_ao
            if fix.get("ao_reason"):
                out["ao_reason"] = fix["ao_reason"]
        except Exception:
            pass

    out["_item_text"] = item_text
    out["_raw"] = raw
    return out


# =========================
# Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„ + Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
# =========================
def compute_percent_match(items: List[Dict]) -> int:
    if not items:
        return 0
    ok = 0
    for it in items:
        lo = str(it.get("learning_objective", "")).strip()
        if lo and lo not in {"-", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯", "ØºÙŠØ± Ù…ØªÙˆÙØ±"}:
            ok += 1
    return int(round(100 * ok / len(items)))

def detect_mcq(item_text: str) -> bool:
    t = item_text or ""
    return bool(re.search(r"(Ø£\)|Ø¨\)|Ø¬\)|Ø¯\))|(\bA\b|\bB\b|\bC\b|\bD\b)", t))

def detect_long_answer(item_text: str) -> bool:
    t = norm_ar(item_text)
    return any(norm_ar(x) in t for x in ["Ù†Ø§Ù‚Ø´", "Ø¨Ø±Ù‡Ù†", "Ø§Ø«Ø¨Øª", "Ø§ÙƒØªØ¨ ØªÙ‚Ø±ÙŠØ±", "Ø¹Ù„Ù„ ØªØ¹Ù„ÙŠÙ„Ø§", "ÙØ³Ø± ØªÙØ³ÙŠØ±Ø§"])

def build_working_table(items: List[Dict]) -> Dict:
    n_items = len(items)
    mcq_count = sum(1 for it in items if detect_mcq(it.get("_item_text", "")))
    long_count = sum(1 for it in items if detect_long_answer(it.get("_item_text", "")))

    a01 = sum(1 for it in items if str(it.get("assessment_objective", "")).strip() == "A01")
    a02 = sum(1 for it in items if str(it.get("assessment_objective", "")).strip() == "A02")
    mix = sum(1 for it in items if str(it.get("assessment_objective", "")).strip() == "A01/A02")

    wt = {
        "Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª": {"value": str(n_items), "status": "Ù…Ø·Ø§Ø¨Ù‚"},
        "Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ø±ÙˆØ³": {"value": "-", "status": "Ù…Ø·Ø§Ø¨Ù‚"},
        "Ø¯Ø±Ø¬Ø§Øª Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… (A01,A02)": {"value": f"A01={a01} | A02={a02} | A01/A02={mix}", "status": "Ù…Ø·Ø§Ø¨Ù‚"},
        "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù…ÙØ±Ø¯Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŸ": {"value": "Ù†Ø¹Ù…" if long_count > 0 else "Ù„Ø§", "status": "Ù…Ø·Ø§Ø¨Ù‚"},
        "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù…ÙØ±Ø¯ØªØ§Ù† Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ØŸ": {"value": "Ù†Ø¹Ù…" if mcq_count >= 2 else "Ù„Ø§", "status": "Ù…Ø·Ø§Ø¨Ù‚"},
        "Ù‡Ù„ Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ (Ø¥Ø¬Ø§Ø¨Ø§Øª Ø®Ø§Ø·Ø¦Ø©) Ù…Ø´ØªØªØ§Øª Ù…Ù†Ø·Ù‚ÙŠØ©ØŸ": {"value": "-", "status": "Ù…Ø·Ø§Ø¨Ù‚"},
        "Ù‡Ù„ ØµÙŠØ§ØºØ© Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆØ­Ø¬Ù… ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø®Ø· ÙˆØ§Ø¶Ø­ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©ØŸ": {"value": "-", "status": "Ù…Ø·Ø§Ø¨Ù‚"},
        "Ù‡Ù„ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„Ø±Ø³ÙˆÙ…Ø§Øª ÙˆØ§Ø¶Ø­Ø©ØŸ": {"value": "-", "status": "Ù…Ø·Ø§Ø¨Ù‚"},
    }
    return wt


# =========================
# Word
# =========================
def _rtl_paragraph(paragraph):
    paragraph.alignment = WD_ALIGN_PARAGRAPH.RIGHT

def _rtl_cell(cell):
    for p in cell.paragraphs:
        _rtl_paragraph(p)

def exam_label_ar(exam_type_value: str) -> str:
    return "Ø§Ù„Ù‚ØµÙŠØ±Ø©" if exam_type_value == "Ù‚ØµÙŠØ±" else "Ø§Ù„Ø§Ø³ØªÙ‚ØµØ§Ø¦ÙŠØ©"

def build_report_docx(data: dict, exam_label: str) -> bytes:
    doc = Document()

    title = f"Ù†Ù…ÙˆØ°Ø¬ ØªÙ‚Ø±ÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª {exam_label}"
    p = doc.add_paragraph(title)
    _rtl_paragraph(p)
    doc.add_paragraph("")

    p = doc.add_paragraph("Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†ÙŠØ©")
    _rtl_paragraph(p)

    headers = [
        "Ø§Ù„Ù…ÙØ±Ø¯Ø©",
        "Ø§Ù„Ù‡Ø¯Ù Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ",
        "Ù‡Ø¯Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… (A01,A02)",
        "Ø§Ù„Ø¯Ø±Ø¬Ø©",
        "Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø© (ØµÙŠØ§ØºØ©ØŒ Ø¹Ù„Ù…ÙŠØ©ØŒ ÙÙ†ÙŠØ© ØªØ´Ù…Ù„ Ø§Ù„Ø±Ø³Ù…)",
        "Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©",
        "Ø§Ù„ØªØ¹Ø¯ÙŠÙ„",
    ]

    items = data.get("items", []) or []
    rows_needed = max(1, len(items)) + 1

    t1 = doc.add_table(rows=rows_needed, cols=len(headers))
    t1.style = "Table Grid"
    t1.alignment = WD_TABLE_ALIGNMENT.RIGHT

    for j, h in enumerate(headers):
        t1.cell(0, j).text = h
        _rtl_cell(t1.cell(0, j))

    for i, item in enumerate(items, start=1):
        t1.cell(i, 0).text = str(item.get("mufrada", "-")).strip()
        t1.cell(i, 1).text = str(item.get("learning_objective", "-")).strip()
        t1.cell(i, 2).text = str(item.get("assessment_objective", "-")).strip()
        t1.cell(i, 3).text = str(item.get("marks", "-")).strip()
        t1.cell(i, 4).text = str(item.get("note_type", "-")).strip()
        t1.cell(i, 5).text = str(item.get("note", "-")).strip()
        t1.cell(i, 6).text = str(item.get("edit", "-")).strip()
        for j in range(len(headers)):
            _rtl_cell(t1.cell(i, j))

    doc.add_paragraph("")

    p = doc.add_paragraph(f"Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± {exam_label}")
    _rtl_paragraph(p)

    wt = data.get("working_table", {}) or {}
    rows_order = [
        "Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª",
        "Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ø±ÙˆØ³",
        "Ø¯Ø±Ø¬Ø§Øª Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… (A01,A02)",
        "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù…ÙØ±Ø¯Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŸ",
        "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù…ÙØ±Ø¯ØªØ§Ù† Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ØŸ",
        "Ù‡Ù„ Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ (Ø¥Ø¬Ø§Ø¨Ø§Øª Ø®Ø§Ø·Ø¦Ø©) Ù…Ø´ØªØªØ§Øª Ù…Ù†Ø·Ù‚ÙŠØ©ØŸ",
        "Ù‡Ù„ ØµÙŠØ§ØºØ© Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆØ­Ø¬Ù… ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø®Ø· ÙˆØ§Ø¶Ø­ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©ØŸ",
        "Ù‡Ù„ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„Ø±Ø³ÙˆÙ…Ø§Øª ÙˆØ§Ø¶Ø­Ø©ØŸ",
    ]

    t2 = doc.add_table(rows=1 + len(rows_order), cols=3)
    t2.style = "Table Grid"
    t2.alignment = WD_TABLE_ALIGNMENT.RIGHT

    t2.cell(0, 0).text = "Ø§Ù„Ø¨Ù†Ø¯"
    t2.cell(0, 1).text = "Ø§Ù„Ø¹Ø¯Ø¯ / Ø§Ù„Ø¯Ø±Ø¬Ø§Øª â€“ Ù†Ø¹Ù… / Ù„Ø§"
    t2.cell(0, 2).text = "Ù…Ø·Ø§Ø¨Ù‚ / ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚"
    for j in range(3):
        _rtl_cell(t2.cell(0, j))

    for i, row_label in enumerate(rows_order, start=1):
        t2.cell(i, 0).text = row_label
        entry = wt.get(row_label, {}) or {}
        t2.cell(i, 1).text = str(entry.get("value", "-")).strip()
        t2.cell(i, 2).text = str(entry.get("status", "-")).strip()
        for j in range(3):
            _rtl_cell(t2.cell(i, j))

    p = doc.add_paragraph(f"Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± {exam_label}")
    _rtl_paragraph(p)

    overall = data.get("overall", {}) or {}
    summary = str(overall.get("summary", "-")).strip()
    percent = overall.get("percent_match", "")

    text = summary
    if percent != "" and percent is not None:
        text = f"{summary}\nÙ†Ø³Ø¨Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ±: {percent}%"
    p = doc.add_paragraph(text.strip())
    _rtl_paragraph(p)

    bio = BytesIO()
    doc.save(bio)
    return bio.getvalue()


# =========================
# Ø¹Ø±Ø¶ HTML Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙØ­Ø©
# =========================
def render_table_html(headers: List[str], rows: List[List[str]]) -> str:
    th = "".join([f"<th>{h}</th>" for h in headers])
    trs = []
    for r in rows:
        tds = "".join([f"<td>{(c if c is not None and str(c).strip() else '-')}</td>" for c in r])
        trs.append(f"<tr>{tds}</tr>")
    return f'<table class="tbl"><thead><tr>{th}</tr></thead><tbody>{"".join(trs)}</tbody></table>'


# =========================
# Ø§Ù„ØªÙ†ÙÙŠØ°
# =========================
run = st.button("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„")

if run:
    if not api_key:
        st.error("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ API Ø£ÙˆÙ„Ù‹Ø§.")
        st.stop()

    if not file_test or not file_policy or not file_book:
        st.error("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«Ø©: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± + ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ… + ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨.")
        st.stop()

    try:
        genai.configure(api_key=api_key)
        model, model_name = pick_model()
        st.sidebar.success(f"âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØªØ§Ø±: {model_name}")

        # Ù†Ø·Ø§Ù‚ Ø§Ù„ØµÙØ­Ø§Øª Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ ÙÙ‚Ø·
        pr_book = _parse_page_range(pages_range)

        exam_label = exam_label_ar(exam_type)

        with st.spinner("Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª (OCR ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©)..."):
            # Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: Ù„Ø§ Ù†Ø·Ø¨Ù‚ Ø¹Ù„ÙŠÙ‡ Ù†Ø·Ø§Ù‚ ØµÙØ­Ø§Øª (ÙŠÙÙ‚Ø±Ø£ ÙƒØ§Ù…Ù„Ù‹Ø§)
            txt_test, mode_test = extract_text_auto(model, file_test.getvalue(), None)

            # ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚ÙˆÙŠÙ…: ØªÙÙ‚Ø±Ø£ ÙƒØ§Ù…Ù„Ø©
            txt_policy, mode_policy = extract_text_auto(model, file_policy.getvalue(), None)

            # ÙƒØªØ§Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨: ÙŠØ·Ø¨Ù‚ Ø¹Ù„ÙŠÙ‡ Ù†Ø·Ø§Ù‚ Ø§Ù„ØµÙØ­Ø§Øª ÙÙ‚Ø·
            txt_book, mode_book = extract_text_auto(model, file_book.getvalue(), pr_book)

            txt_test = safe_clip(txt_test, 110000)
            txt_policy = safe_clip(txt_policy, 150000)
            txt_book = safe_clip(txt_book, 120000)

        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±..."):
            items_base = extract_items_via_llm(model, txt_test)

        if not items_base:
            with st.spinner("ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ. Ø³Ø£Ø¬Ø±Ø¨ OCR Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø«Ù… Ø£Ø¹ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬..."):
                txt_test_ocr = ocr_pdf_with_gemini(model, file_test.getvalue(), None, max_pages=25)
                if txt_test_ocr.strip():
                    txt_test = safe_clip(txt_test_ocr, 110000)
                    items_base = extract_items_via_llm(model, txt_test)

        if not items_base:
            st.error("Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙØ±Ø¯Ø§Øª Ù…Ù† Ù…Ù„Ù Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø­ØªÙ‰ Ø¨Ø¹Ø¯ OCR.")
            st.stop()

        analyzed_items = []
        prog = st.progress(0)
        total = len(items_base)

        for idx, it in enumerate(items_base, start=1):
            with st.spinner(f"ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø© {it.get('number')} ..."):
                analyzed_items.append(analyze_one_item(model, it, txt_policy, txt_book))
            prog.progress(int(100 * idx / total))

        percent_match = compute_percent_match(analyzed_items)
        working_table = build_working_table(analyzed_items)

        compact = [
            {
                "mufrada": x.get("mufrada"),
                "assessment_objective": x.get("assessment_objective"),
                "note_type": x.get("note_type"),
                "note": x.get("note"),
            }
            for x in analyzed_items
        ]

        overall_prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªÙ‚ÙˆÙŠÙ…. Ø§ÙƒØªØ¨ ØªÙ‚Ø¯ÙŠØ±Ù‹Ø§ Ø¹Ø§Ù…Ù‹Ø§ Ù…Ø®ØªØµØ±Ù‹Ø§ (3-5 Ø£Ø³Ø·Ø±) Ø¹Ù† Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± {exam_label}.
Ø±ÙƒØ² Ø¹Ù„Ù‰: ØªÙˆØ§Ø²Ù† A01/A02 ÙˆÙÙ‚ Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø±Ø³Ù…ÙŠØŒ Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© ÙˆØ§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©ØŒ ÙˆÙ…ÙˆØ§Ø·Ù† Ø§Ù„ØªØ­Ø³ÙŠÙ†.
Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… " Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ.

Ø£Ø®Ø±Ø¬ JSON ÙÙ‚Ø·:
{{"summary":"..."}}

Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªØµØ±Ø©:
{json.dumps(compact, ensure_ascii=False)}
"""
        overall_data, _ = generate_json(model, overall_prompt, tries=2)
        overall_summary = str(overall_data.get("summary", "")).strip() or "-"

        report_data = {
            "items": analyzed_items,
            "working_table": working_table,
            "overall": {"summary": overall_summary, "percent_match": percent_match},
        }

        st.markdown("---")
        st.subheader("Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†ÙŠØ©")

        headers1 = [
            "Ø§Ù„Ù…ÙØ±Ø¯Ø©",
            "Ø§Ù„Ù‡Ø¯Ù Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ",
            "Ù‡Ø¯Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… (A01,A02)",
            "Ø§Ù„Ø¯Ø±Ø¬Ø©",
            "Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø© (ØµÙŠØ§ØºØ©ØŒ Ø¹Ù„Ù…ÙŠØ©ØŒ ÙÙ†ÙŠØ© ØªØ´Ù…Ù„ Ø§Ù„Ø±Ø³Ù…)",
            "Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©",
            "Ø§Ù„ØªØ¹Ø¯ÙŠÙ„",
        ]

        rows1 = []
        for it in analyzed_items:
            rows1.append([
                str(it.get("mufrada", "-")).strip(),
                str(it.get("learning_objective", "-")).strip(),
                str(it.get("assessment_objective", "-")).strip(),
                str(it.get("marks", "-")).strip(),
                str(it.get("note_type", "-")).strip(),
                str(it.get("note", "-")).strip(),
                str(it.get("edit", "-")).strip(),
            ])

        st.markdown(render_table_html(headers1, rows1), unsafe_allow_html=True)

        st.markdown("")
        st.subheader(f"Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± {exam_label}")

        headers2 = ["Ø§Ù„Ø¨Ù†Ø¯", "Ø§Ù„Ø¹Ø¯Ø¯ / Ø§Ù„Ø¯Ø±Ø¬Ø§Øª â€“ Ù†Ø¹Ù… / Ù„Ø§", "Ù…Ø·Ø§Ø¨Ù‚ / ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚"]
        rows2 = []
        rows_order = [
            "Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª",
            "Ø¹Ø¯Ø¯ Ø§Ù„Ø¯Ø±ÙˆØ³",
            "Ø¯Ø±Ø¬Ø§Øª Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªÙ‚ÙˆÙŠÙ… (A01,A02)",
            "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù…ÙØ±Ø¯Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŸ",
            "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù…ÙØ±Ø¯ØªØ§Ù† Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ØŸ",
            "Ù‡Ù„ Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ù…ØªØ¹Ø¯Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ (Ø¥Ø¬Ø§Ø¨Ø§Øª Ø®Ø§Ø·Ø¦Ø©) Ù…Ø´ØªØªØ§Øª Ù…Ù†Ø·Ù‚ÙŠØ©ØŸ",
            "Ù‡Ù„ ØµÙŠØ§ØºØ© Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆØ­Ø¬Ù… ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø®Ø· ÙˆØ§Ø¶Ø­ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©ØŸ",
            "Ù‡Ù„ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„Ø±Ø³ÙˆÙ…Ø§Øª ÙˆØ§Ø¶Ø­Ø©ØŸ",
        ]
        for k in rows_order:
            entry = working_table.get(k, {}) or {}
            rows2.append([k, str(entry.get("value", "-")), str(entry.get("status", "-"))])

        st.markdown(render_table_html(headers2, rows2), unsafe_allow_html=True)

        st.markdown("")
        st.subheader(f"Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± {exam_label}")
        st.markdown(
            f'<div class="report-box">{overall_summary}<br><br>Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ±: {percent_match}%</div>',
            unsafe_allow_html=True
        )

        with st.expander("Ø¹Ø±Ø¶ Ù†Øµ ÙƒÙ„ Ù…ÙØ±Ø¯Ø© + Ø³Ø¨Ø¨ ØªØµÙ†ÙŠÙ A01/A02 (Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©)"):
            for it in analyzed_items:
                st.markdown(f"**Ø§Ù„Ù…ÙØ±Ø¯Ø© {it.get('mufrada')}**")
                st.write(it.get("_item_text", "-"))
                st.markdown(f"<div class='muted'>Ø³Ø¨Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ: {it.get('ao_reason','-')}</div>", unsafe_allow_html=True)
                st.markdown("---")

        docx_bytes = build_report_docx(report_data, exam_label)
        st.download_button(
            "ğŸ“¥ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± (Word)",
            data=docx_bytes,
            file_name="Report.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")
